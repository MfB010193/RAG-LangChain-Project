{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba99ef65",
   "metadata": {},
   "source": [
    "# **Document-Based RAG System Using LangChain + FAISS**\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Project Title**\n",
    "**Build a Document-Based RAG System Using LangChain and FAISS**\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Objective**\n",
    "The goal of this project is to build a **Retrieval-Augmented Generation (RAG)** system that can answer questions about a private or domain-specific document that the LLM alone cannot answer correctly.\n",
    "\n",
    "**Key Objectives:**\n",
    "- Handle private/internal documents\n",
    "- Improve answer accuracy using document retrieval\n",
    "- Demonstrate the difference between **LLM-only answers** and **RAG-based answers**\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Document Used**\n",
    "**File:** `Concept Note.pdf`  \n",
    "**Type:** Internal research / project proposal document  \n",
    "**Reason:** Contains domain-specific content that LLMs cannot answer without retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. RAG Pipeline Overview**\n",
    "The RAG system works in these steps:\n",
    "\n",
    "\n",
    "[PDF Document] → [Chunking] → [Embeddings] → [FAISS Vector DB] → [Retriever] → [Prompt Template] → [LLM] → [Answer]\n",
    "\n",
    "\n",
    "1. Load the document  \n",
    "2. Split the document into chunks  \n",
    "3. Generate embeddings for each chunk using `HuggingFaceEmbeddings`  \n",
    "4. Store embeddings in **FAISS vector database**  \n",
    "5. Retrieve top-k relevant chunks for a query  \n",
    "6. Feed retrieved content into **LLM via prompt template**  \n",
    "7. Generate final answer  \n",
    "\n",
    "**Result:** Answers are **based only on retrieved context**, avoiding hallucinations.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Demo Queries and Answers**\n",
    "\n",
    "### **Query 1:** What is the main purpose of the Concept Note?\n",
    "- **RAG Answer:**  \n",
    "  > The main purpose of the Concept Note is to develop a Physics-Informed Neural Network (PINN) model that integrates with physical constraints derived from hydrological and atmospheric equations governing rainfall processes for accurate rainfall projections in the Blue Nile Basin, Ethiopia.\n",
    "- **LLM-only Answer:**  \n",
    "  > A Concept Note is a document used in various contexts, but its main purpose remains the same in most cases. The main purpose of a Concept Note is to provide a brief overview and preliminary outline of a project, proposal, or idea. It serves as a precursor to a more detailed proposal or plan.  \n",
    "  > *(Includes general description, methodology, budget, and objectives—mostly generic and not specific to the project.)*\n",
    "\n",
    "---\n",
    "\n",
    "### **Query 2:** List the 3 implementation steps mentioned in the Concept Note.\n",
    "- **RAG Answer:**  \n",
    "  1. Implement a PINN (Physics-Informed Neural Network) that integrates with physical constraints derived from hydrological and atmospheric equations governing rainfall process.  \n",
    "  2. Compare performance with conventional ML models like LSTMs, Random Forests, and purely physics-based downscaling methods.  \n",
    "  3. Implement the PINN using the supplementary datasets: CHIPRS (Climate Hazard Group InfraRed Precipitation with Station data) or GPM (Global Precipitation Measurement).  \n",
    "- **LLM-only Answer:**  \n",
    "  > Provides a general outline of what a Concept Note usually contains, but **does not specify the exact implementation steps**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Query 3:** What budget items are mentioned in the Concept Note?\n",
    "- **RAG Answer:**  \n",
    "  > There are no budget items mentioned in the context provided. The document focuses on objectives, methodology, and practical contributions.  \n",
    "- **LLM-only Answer:**  \n",
    "  > Mentions budget as part of a generic Concept Note, but this is **hallucinated** because the actual document does not include any budget items.  \n",
    "\n",
    "---\n",
    "\n",
    "## **6. Design Choices**\n",
    "\n",
    "| Component | Choice | Reason |\n",
    "|-----------|--------|--------|\n",
    "| Chunking | 800 characters, 100 overlap | Preserve context while staying within LLM limits |\n",
    "| Embeddings | `sentence-transformers/all-MiniLM-L6-v2` | Lightweight semantic embeddings |\n",
    "| Vector Store | FAISS | Fast local similarity search |\n",
    "| LLM | `ChatGroq llama-3.1-8b-instant` | Supports prompt-based retrieval |\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Challenges & Solutions**\n",
    "\n",
    "| Challenge | Solution |\n",
    "|-----------|---------|\n",
    "| Large documents | Split into smaller chunks for retrieval |\n",
    "| Retrieval relevance | Used semantic embeddings with FAISS |\n",
    "| LLM hallucination | Restricted LLM to **only retrieved context** |\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Key Takeaways**\n",
    "- RAG improves **accuracy** for private or domain-specific content  \n",
    "- Retrieval ensures answers are **grounded in the document**, preventing hallucinations  \n",
    "- LLM-only answers can be **generic or hallucinated**, highlighting the importance of RAG  \n",
    "- LangChain + FAISS provide an **efficient and modular pipeline**  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375d487f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
